
paths:
  checkpoint_dir: checkpoints   # Directory to store model checkpoints and tensorboard, will be created if not existing.
  data_dir: datasets            # Directory to store processed data, will be created if not existing.
preprocessing:
  languages: ['tj']    # All languages in the dataset.
  # Text (grapheme) and phoneme symbols, either provide a string or list of strings.
  # Symbols in the dataset will be filtered according to these lists!

  text_symbols: ['а','б','в','г','ғ','д','е','ё','ж','з','и','й','ӣ','қ','к','л','м','н','о','п','р','с','т','у','ӯ','ф','х','ҳ','ч','ҷ','ш','ъ','э','ю','я']
  phoneme_symbols: ['ض','آ','ب','د','گ','غ','ح','ج','چ','ک','ق','ل','م','ن','پ','ر','ش','ذ','ص','ت','خ','ز','ع','ژ','ظ','ط','س','ث','ف','ا','و','ی','ه','‌','ِ']
  char_repeats: 3                # Number of grapheme character repeats to allow for mapping to longer phoneme sequences.                         # Set to 1 for autoreg_transformer.
  lowercase: true                # Whether to lowercase the grapheme input.
  n_val: 5600                    # Default number of validation data points if no explicit validation data is provided.

model:
  type: 'transformer'            # Whether to use a forward transformer or autoregressive transformer model.                               # Choices: ['transformer', 'autoreg_transformer']
  d_model: 128
  d_fft: 512
  layers: 4
  dropout: 0.1
  heads: 8

training:
  learning_rate: 0.0002              # Learning rate of Adam.
  warmup_steps: 200                   # Linear increase of the lr from zero to the given lr within the given number of steps.
  scheduler_plateau_factor: 0.5      # Factor to multiply learning rate on plateau.
  scheduler_plateau_patience: 1    # Number of text generations with no improvement to tolerate.
  batch_size: 32                     # Training batch size.
  batch_size_val: 32                 # Validation batch size.
  epochs: 200                      # Number of epochs to train.
  generate_steps: 10000                # Interval of training steps to generate sample outputs. Also, at this step the phoneme and word                         # error rates are calculated for the scheduler.
  validate_steps: 10000                # Interval of training steps to validate the model                     # (for the autoregressive model this is teacher-forced).
  checkpoint_steps: 50000             #    Interval of training steps to save the model.
  n_generate_samples: 10             # Number of result samples to show on tensorboard.
  store_phoneme_dict_in_model: true  # Whether to store the raw phoneme dict in the model.
  ddp_backend: 'nccl'                # Backend used by Torch DDP
  ddp_host: 'localhost'              # Hostname used by Torch DDP
  ddp_post: '12355'                    # Port used by Torch DDP                       

